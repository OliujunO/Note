# 机器学习

## 迭代算法

### 梯度下降算法
$\theta := \theta + \alpha \Delta _\theta l( \theta )$

参数`θ`在每次迭代后等于`θ`加上`学习率θ`($\alpha$)乘以`成本函数的梯度`($ \Delta _\theta l(\theta)$)

## 分类算法

### 二元分类

#### 线性回归的效果很不好
线性回归方法受每一个样本影响改变回归线的斜率。
回归线两边的样本数如果较不对称，则很容易导致回归线偏移，影响线性回归的分类效果。

#### Signoid函数

##### 公式
$g (z) = \frac { 1 } {1 + e ^ { -z } }$ 

##### 图像
![Signoid](http://52opencourse.com/?qa=blob&qa_blobid=4490461754688732968)

##### 应用于二元分类
$P (y = 1 | x ; \theta) = \frac { 1 } { 1 + e ^ { \theta ^ T _ x } }$

假设：在给定的条件`θ`下，`x`使`y = 1`的概率符合`Signoid`函数分布
则：可以得出根据`x`推测`y`为`0`或`1`的概率
即：根据`x`可以对`y`进行二元分类，推测为`0`或`1`的概率。

相较于`线性回归`方法，这种方法更为平滑可靠
